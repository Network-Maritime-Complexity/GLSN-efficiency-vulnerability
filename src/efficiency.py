import networkx as nx
import csv
import random
import pandas as pd
from heapq import heappush, heappop
from collections import deque
import os
import copy

def foundnetwork():
    df = pd.read_excel("../data/Edge 2018_with great circle distance (GC) and weight.xlsx", sheet_name='2018 edge GC and weight')
    G = nx.Graph()
    for index, row in df.iterrows():
        G.add_edge(row['port1'], row['port2'], weight=row['weight'],GC=row['GC'])

    print("number of nodes:", G.number_of_nodes())
    print("number of edges:", G.number_of_edges())
    return G

def efficiency_measurement(G, G0,u=None,n = 887,path1="../data/min efficiency between each pair of ports in all realistic routes.csv",path2="../data/distance between each pair of ports.csv"):
    '''
    G: network under disruptions of u
    G0: the original network
    n: the number of ports in G0
    u: the disrupted port or country
    path1: the path of file stored for min efficiency between each pair of ports in G0
    path2: the path of file stored for shortest real distance between each pair of ports in G0
    '''
    we=0
    df0 = pd.read_csv(path1)
    df0['port1'] = df0['port1'].astype(str)
    df0['port2'] = df0['port2'].astype(str)
    df = pd.read_csv(path2)
    df['port1'] = df['port1'].astype(str)
    df['port2'] = df['port2'].astype(str)
    df = pd.merge(df, df0, on=['port1', 'port2'])
    GC = nx.Graph()
    for index, row in df.iterrows():
        GC.add_edge(row['port1'], row['port2'], GC=row['GC'], E=row['e'])
    length = dict(nx.all_pairs_shortest_path_length(G0))
    nodes = sorted(G.nodes)
    for s in nodes:
        # single source shortest paths
        # use BFS
        S, P, sigma, _ ,Gv,mindistance,w,e= _single_source_shortest_path_basic(G, s,GC,length,u)

        # accumulation
        eij = _accumulate_basic(S, P, sigma, s, e,u)
        we += eij

    return we / n / (n - 1)

def _single_source_shortest_path_basic(G, s,GC,length,u):
    S = []
    P = {}
    Gv = {}
    sps = {}

    for v in G:
        P[v] = []
        sps[v]=[]
        # Gv[v]=nx.Graph()
    sigma = dict.fromkeys(G, 0.0)  # sigma[v]=0 for v in G
    D = {}
    Path = dict.fromkeys(G, '')
    totalweight = dict.fromkeys(G, 0.0)
    weight = dict.fromkeys(G, 0.0)
    ei = dict.fromkeys(G, 0.0)
    distance = dict.fromkeys(G, 100000000)
    sigma[s] = 1.0
    Path[s] = s
    D[s] = 0
    totalweight[s] = 0
    weight[s] = 0
    ei[s] = 0
    distance[s] = 0
    sps[s]=[(0,0)]
    Q = deque([s])
    while Q:  # use BFS to find shortest paths
        v = Q.popleft()
        S.append(v)
        Dv = D[v]
        totalweightv = totalweight[v]
        Pathv = Path[v]
        sigmav = sigma[v]
        for w in G[v]:
            if w not in D:
                Q.append(w)
                D[w] = Dv + 1  # the length of shortest path
            if D[w] == Dv + 1:  # this is a shortest path, count paths
                sigma[w] += sigmav  # the number of shortest path
                totalweight[w] += totalweightv + G.get_edge_data(w, v)['weight']
                P[w].append(v)  # predecessors
                if (length[w][s] == D[w]):
                    for sp in sps[v]:
                        sps[w].append((sp[0]+1 / G.get_edge_data(w, v)['weight'],sp[1]+G.get_edge_data(w, v)['GC']))
                        if ((sp[1] + G.get_edge_data(w, v)['GC']) <= 1.05 * (GC.get_edge_data(w, s)['GC'])):
                            tmpweight = sp[0] + (1 / G.get_edge_data(w, v)['weight'])
                            tmpei = 1 / tmpweight
                            if (tmpei > ei[w]):
                                weight[w] = tmpweight
                                ei[w] = tmpei
                                distance[w] = sp[1] + G.get_edge_data(w, v)['GC']
                                Path[w] = Pathv + ',' + w

    for node in G:
        if(ei[node]==0 and node!=s):
            ei[node] = GC.get_edge_data(node, s)['E']
    if (u==None and G.number_of_nodes() == GC.number_of_nodes()):
        df = pd.DataFrame(ei, index=[0]).T
        df['port1'] = s
        df.to_csv('../output/efficiency between each pair of ports.csv', header=0,mode='a+')
    return S, P, sigma, D, Gv,distance,weight,ei

def _accumulate_basic(S, P, sigma, s,e,u):
    delta = dict.fromkeys(S, 0)
    eij=0
    S=sorted(S)
    outer=[]
    while S:
        w = S.pop()
        coeff = (1 + delta[w]) / sigma[w]

        for v in P[w]:
            delta[v] += sigma[v] * coeff
        if w != s:
            eij += e[w]
            outer.append([s,w,e[w]])

    if (u != None and u !='dismantle'):
        pd.DataFrame(outer).to_csv('../output/efficiency between each pair of ports under disruption of ' + u + '.csv',header=0,mode='a+')

    return eij

def port_efficiency():
    df=pd.read_csv('../output/efficiency between each pair of ports.csv',names=['port1','e','port2'])
    df=df.groupby(by='port1',as_index=False)['e'].sum()
    print(df)
    df['e']=df['e']/886
    df.to_csv('../output/port efficiency.csv',index=False)

def country_efficiency():
    df=pd.read_csv('../output/port efficiency.csv')
    ports=pd.read_excel('../data/port 2018.xlsx')
    df=pd.merge(df,ports,left_on='port1',right_on='port')
    df=df.groupby(by='Country Name',as_index=False)['e'].sum()
    df.to_csv('../output/country efficiency.csv',index=False)

def main():
    #the efficiency of original network
    G0=foundnetwork()
    print(efficiency_measurement(G0,G0))
    port_efficiency()
    country_efficiency()

